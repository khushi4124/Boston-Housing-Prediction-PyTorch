# -*- coding: utf-8 -*-
"""Boston Housing Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cAOqp3bHepc8bmSke0-b6k6HeZAqyJN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv("/content/BostonHousingData.csv")

df.head()

df.shape

df.describe()

"""# **UNIVARIATE ANALYSIS**

UNIVARIATE ANALYSIS OF CRIM
"""

df['CRIM'].isnull().sum()

df['CRIM'].plot(kind='kde')

df['CRIM'].skew()

df['CRIM']=df['CRIM'].fillna(df['CRIM'].median())

#log transformation as the distribution is right skewed
df['CRIM']=np.log1p(df['CRIM'])

df['CRIM'].skew()

df['CRIM'].plot(kind='box')

#checking if the outliers actually exist or not
df[df['CRIM']>3.4]

"""UNIVARIATE ANALYSIS OF ZN"""

df['ZN'].isnull().sum()

df['ZN']=df['ZN'].fillna(df['ZN'].median())

df['ZN'].plot(kind='kde')

df['ZN'].skew()

#log transformation as the distribution is right skewed
df['ZN']=np.log1p(df['ZN'])

#skewness after transformation
df['ZN'].skew()

"""UNIVARIATE ANALYSIS OF INDUS"""

df['INDUS'].isnull().sum()

df['INDUS'].plot(kind='kde')

df['INDUS'].skew()

df['INDUS']=df['INDUS'].fillna(df['INDUS'].median())

df['INDUS'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF CHAS"""

df['CHAS'].isnull().sum()

df['CHAS'].value_counts()

df['CHAS']=df['CHAS'].fillna(df['CHAS'].median())

df['CHAS'].isnull().sum()

"""UNIVARIATE ANALYSIS OF NOX"""

df['NOX'].isnull().sum()

df['NOX'].plot(kind='kde')

df['NOX'].skew()

df['NOX'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF RM"""

df['RM'].isnull().sum()

df['RM'].plot(kind='kde')

df['RM'].skew()

df['RM'].plot(kind='box')

#checking if the outliers are actually there or not
df[df['RM']>7.5]

#checking if the outliers are actually there or not
df[df['RM']<4.7]

"""UNIVARIATE ANALYSIS OF AGE"""

df['AGE'].isnull().sum()

df['AGE']=df['AGE'].fillna(df['AGE'].median())

df['AGE'].plot(kind='kde')

df['AGE'].skew()

df['AGE'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF DIS"""

df['DIS'].isnull().sum()

df['DIS'].plot(kind='kde')

df['DIS'].skew()

#log transformation as the distriibution is right skewed
df['DIS']=np.log1p(df['DIS'])

#skewness after the log transformation
df['DIS'].skew()

df['DIS'].plot(kind='kde')

"""UNIVARIATE ANALYSIS OF RAD"""

df['RAD'].isnull().sum()

df['RAD'].plot(kind='kde')

df['RAD'].skew()

#log transformation as the distribution is right skewed
df['RAD']=np.log1p(df['RAD'])

#skewness after the transformation
df['RAD'].skew()

df['RAD'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF TAX"""

df['TAX'].isnull().sum()

df['TAX'].plot(kind='kde')

df['TAX'].skew()

df['TAX'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF PTRATIO"""

df['PTRATIO'].isnull().sum()

df['PTRATIO'].plot(kind='kde')

df['PTRATIO'].skew()

df['PTRATIO'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF B"""

df['B'].isnull().sum()

df['B'].plot(kind='kde')

df['B'].skew()

#left skewed distribution so applying reflect + log transformation
df['B']=np.log1p(df['B'].max()+1-df['B'])

#skewness after the transformation
df['B'].skew()

df['B'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF LSTAT"""

df['LSTAT'].isnull().sum()

df['LSTAT'].plot(kind='kde')

df['LSTAT'].skew()

df['LSTAT']=df['LSTAT'].fillna(df['LSTAT'].median())

df['LSTAT'].skew()

#slightly right skewed distribution, so applying log transformation
df['LSTAT']=np.log1p(df['LSTAT'])

#skewness after transformation
df['LSTAT'].skew()

df['LSTAT'].plot(kind='box')

"""UNIVARIATE ANALYSIS OF MEDV"""

df['MEDV'].isnull().sum()
# no need of further analysis as this is the target variable

"""X AND Y"""

#creating input features dataset/dataframe
X=df.drop(columns=['MEDV'])

X

#creating target dataset/dataframe
Y=df['MEDV']

Y

"""USING PYTORCH TO BUILD OUR MODEL"""

import torch
import torch.nn as nn

import torch.optim as optim

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

X_train, X_test, Y_train, Y_test=train_test_split(X_scaled,Y,test_size=0.2,random_state=42)

X_train_tensor=torch.tensor(X_train, dtype=torch.float32)
Y_train_tensor=torch.tensor(Y_train.values, dtype=torch.float32).view(-1,1)
Y_test_tensor=torch.tensor(Y_test.values, dtype=torch.float32).view(-1,1)
X_test_tensor=torch.tensor(X_test, dtype=torch.float32)

class LinearRegression(nn.Module):
  def __init__(self,in_features,out_features):
    super().__init__()
    self.layer1=nn.Linear(in_features,32)
    self.dropout=nn.Dropout(0.2)     #randomly zeroes 20 neurons to prevent overfitting
    self.relu=nn.ReLU()   #activation function to introduce non linearity
    self.layer2=nn.Linear(32,out_features)
  def forward(self,X_train_tensor):
    X_train_tensor=self.layer1(X_train_tensor)
    X_train_tensor=self.dropout(X_train_tensor)
    X_train_tensor=self.relu(X_train_tensor)
    X_train_tensor=self.layer2(X_train_tensor)
    return X_train_tensor

model=LinearRegression(in_features=13,out_features=1)
print(model)

learning_rate=0.01
optimizer=optim.Adam(model.parameters(),lr=learning_rate)
lossfunction=nn.MSELoss()

epochs=1000
for epoch in range(epochs):
  y_predicted=model(X_train_tensor)
  loss=lossfunction(y_predicted,Y_train_tensor)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  if (epoch%100==0):
    print(f"epoch {epoch}, loss {loss}")
print(f"epoch 1000, loss {loss}")

#testing
model.eval()
with torch.no_grad():
  output=model(X_test_tensor)
  loss=lossfunction(output,Y_test_tensor)
  print(f"loss {loss}")
#since overall training loss ~ test loss, the model did not overfit and was able to generalize the data